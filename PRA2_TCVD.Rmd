---
title: "Tipología y ciclo de vide de los datos: Pr2 - Como realizar la limpieza y análisis de datos"
author: "Alejandro Hernández Slamerón, Almir Cáceres Barraquero"
date: "Junio 2023"
output: 
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Descripción del dataset 



# Integración y seleccion

Ahora vamos a ver los datos de que se dispone para realizar un primer estudio sobre las posibles decisiones previas que se han de tomar. 
Para ello cargamos el dataset y revisamos todos las variables que disponemos, para verificar cual podemos utilizar en estudios posteriores.

```{r message= FALSE, warning=FALSE}
# IMPORTACIÓN DE LIBRERÍAS

if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
if (!require('Hmisc')) install.packages('Hmisc'); library('Hmisc')
if(!require('Rmisc')) install.packages('Rmisc'); library('Rmisc')
```

```{r}
path = 'heart.CSV'
df_heart <- read.csv(path, row.names=NULL)
```

```{r}
head(df_heart)
```



Vemos que disponemos de las siguientes variables para el estudio:


+ **age** Age of the patient in years
+ **sex** Sex of the patient
+ **cp** chest pain type:
(1= typical angina, 
2= atypical angina, 
3= non-anginal, 
4= asymptomatic)
+ **trtbps** resting blood pressure (in mm Hg)
+ **chol**  cholestoral in mg/dl fetched via BMI sensor
+ **fbs** fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
+ **restecg** resting electrocardiographic results 
Value 0: normal,
Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV),
Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
+ **thalachh**	 maximum heart rate achieved
+ **exan**	exang: exercise induced angina (1 = yes; 0 = no)
+ **oldpeak** ST depression induced by exercise relative to rest	
+ **slp** the slope of the peak exercise ST segment
+ **caa** 	number of major vessels (0-3) colored by fluoroscopy
+ **thall** 	3normal; fixed defect; reversible defect
+ **output** the predicted attribute  (0 = less chance of heart attack, 1= more chance of heart attack)

Ahora se va a revisar que tipo de variable es cada una. 

```{r}
structure = str(df_heart)
```

Con esto tenemos la información acerca de la cantidad de datos de los que disponemos y su estructura. 
Como se puede observar, existen 14 variables y un total de 303 datos de estas variables.
Además podemos concluir que todas las variables menos **oldpeak** son variables integrales, lo que nos es muy util para la toma de decisión del tipo de estudio a realizar. 


# Limpieza de datos

Para poder iniciar el análisis de los datos, lo primero es revisar y arreglar los datos donde puedan faltar algún tipo de valores o estos sean erroneos (o puedan dar lugar a resultados erroneo)

## Gestion de elementos erroneos

En primer lugar buscamos todos los datos que tengan alguna variable nulla o vacía. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
print('Blancos')
colSums(df_heart=="")
print('NA')
colSums(is.na(df_heart))
```


Ahora vamos a reviasar los valores de las variables para comprobar si alguna de ellos tuviese valores erroneos. 


```{r}
summary(df_heart)

```



+ **caa**: Esta variable nos dicen en la descripción del dataset que toma valores entre el 0 y el 3. Viendo que su valor máximo es 4, deberíamos eliminar los datos que tengan un valor 4 en esta variable y considerarlos erroneos. 


Eliminamos los valores de caa referentes a 4


```{r message= FALSE, warning=FALSE}

df_heart <- df_heart[!df_heart$caa == 4, ]

```

## Identificación y gestión de valores exremos

Hacemos una última confirmación con el comando describe, para comprobar si hemos de revisar alguna variable más que pueda tener valores anómalos

```{r}
describe(df_heart)
```


+ **oldpeack**: El valor máximo de esta variable es mucho más alto que la media. Esto no quiere deceir que sera erroneo, pero es necesario revisar si estos valores altos son correctos. 

+ **chol**: Al igual que el anterior, el valor máximo de esta variable es mucho más alto que la media. Volvemos a hacer la misma suposición que para la variable anterior, ya que aunque sabemos que estos valores pueden ser altos en ciertas personas, no conocemos exáctamente si estos valores son correctos y pueden afectar al los resultados del estudio. 


Revisamos gráficamente la variable **oldpeak** para ver si tiene valores extremos.


```{r}
boxplot(df_heart$oldpeak, horizontal = TRUE, outline=TRUE)
```


Podemos ver que exsisten 4 valores atípicos que tienen valores más altos de lo normal. Aunque desconocemos si teoricamente estos valores son o no correctos, podemos asumir que son valores anómalos y extraerlos del cálculo final, ya que no va a afectar considerablemente al resultado final. 

```{r message= FALSE, warning=FALSE}
df_heart <- df_heart[!df_heart$oldpeak >= 4, ]

```

Gráficamos también la variable **chol**

```{r}
boxplot(df_heart$chol, horizontal = TRUE, outline=TRUE)
```


Al igual que con la variable **oldpeak**, estos valores no están significativamente separados. Aun así, se van a eliminar los valores anómalos. 

```{r message= FALSE, warning=FALSE}
df_heart <- df_heart[!df_heart$chol >= 400, ]

```
# Análisis de los datos

## Selección de los datos

Una vez hecha la limpieza de datos, se pasa al proceso de análisis.
En primer lugar se ha de definir cual es la finalidad que se busca a la hora del análisis. Para este caso, existe un valor de **output**, el cual toma valores de 0 y 1 y define menor o mayor probabilidad de tener enfermedades cardíacas respectivamente. 

Para decidir el resto de variables que vamos a estudiar, vamos a revisar la distribución de las variables y la correlación que tienen entre ellas. Por ello y en lo referente a este estudio, se van a utilizar las variables cuantitativas, evitando escoger las variables con valores categóricos, las cuales se van a apartar para posibles estudios futuros más efectivos para este tipo de variables como los arboles de decisiones.


Por ello, las variables que se van a estudiar son las siguientes:

+ **age** Podemos intuir que esta variable puede estar muy relacionada con las enfermedades cardíacas, ya que edades avanzadas se relacionan con aumentos de casos de este tipo
+ **trtbps** La presión sanginea tambien es una variable a tener en cuenta, puesto que altas presiones pueden derivar de/o problemas cardíacos. 
+ **chol**  El colesterol es un factor también a tener en cuenta puesto que es un medidor muy utilizado para validar la salud de un paciente. 
+ **thalachh**	No tenemos mucha información sobre esta variable, aún así podemos verificar si puede o no estar relacionada con este tipo de enfermedades.
+ **oldpeak** Esta variable también la vamos a tener en cuenta puesto que sería un punto muy favorable tener un indicador directo de la medida de ritmo cardíaco para poder interpretar este tipo de dolencias cardíacas.
+ **outpu** Este es el valor predicho que indica si existe riesgo o no sobre las dolencias cardíacas. 


## Comprobación de la normalidad y homogeneidad de la varianza


Ahroa se va a comprobar la distribución de las variables, graficando estas para visualizar si siguen o no una distribución normal.


```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=10, fig.height=20}

histList<- list()

n = c("age","trtbps","chol","thalachh", "oldpeak")
df_heart_plot = df_heart %>% select(all_of(n))
for(y in 1:ncol(df_heart_plot)){
  col <- names(df_heart_plot)[y]
  ggp <- ggplot(df_heart_plot, aes_string(x = col)) +
  geom_histogram(bins = 30, fill = "cornflowerblue", color = "black",ggtittle = "Contador de ocurrencias por variable")
  histList[[y]] <- ggp # añadimos cada plot a la lista vacía
}
multiplot(plotlist = histList, coles = 1)

```


Viendo las gráficas podemos observar que la variable **oldpeak** no sigue una distribución normal. Para el resto necesitaríamos utilizar distintas herramientas para confirmar la asunción o no de normalidad de los datos.


